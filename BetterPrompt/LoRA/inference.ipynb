{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99903c7307c47d1867b18a79686f2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "#os.environ[\"https_proxy\"] = 'http://127.0.0.1:7890'\n",
    "#os.environ[\"http_proxy\"] = 'http://127.0.0.1:7890'\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = 'cuda'\n",
    "checkpoint = \"THUDM/chatglm-6b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True, revision = 'main')\n",
    "model = AutoModel.from_pretrained(checkpoint, trust_remote_code=True, revision = 'main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import loralib as lora\n",
    "from lora_utils.insert_lora import get_lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = {\n",
    "        'r': 8,\n",
    "        'lora_alpha':16,\n",
    "        'lora_dropout':0.1,\n",
    "        'enable_lora':[True, False, True],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:24<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable_params:3670016 (0.06%), non_trainable_params:6173286400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = get_lora_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.load_state_dict(torch.load('saved/chatglm-6b_demo.pt'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.half().cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "role = '峰哥'\n",
    "\n",
    "question = f'{role}能锐评一下大语言模型吗？'\n",
    "\n",
    "emotional = '真诚的'\n",
    "length = '详细的'\n",
    "\n",
    "text=f'{question}\\n{role}{emotional}{length}答：'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, padding=True, truncation=True, max_length=1024, return_tensors=\"pt\")\n",
    "inputs = {k: v.cuda() for k,v in inputs.items() }\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    max_length=1024,\n",
    "    eos_token_id=130005,\n",
    "    do_sample=True,\n",
    "    temperature=0.75,\n",
    "    top_p = 0.75,\n",
    "    top_k = 10000,\n",
    "    repetition_penalty=1.5, \n",
    "    num_return_sequences=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 那必须得说啊,我觉得那玩意儿就是挺有用的。毕竟现在人说话都越来越抽象了嘛。就比如说咱们这个日常交流当中经常容易误解别人的意思呀、你听不明白对方的话儿对吧是不是都是这样的所以这个东西确实是非常有用的东西\n",
      " 大家好,我是聊天记录君。我呢是学计算机的啊现在在创业做聊天机器人呀、人工智能做的挺好的吧,然后我写了很多文章嘛也是受很多媒体推荐的呗人家就是给人当评论家啥的说的挺不错我都要写俩篇儿了都成我的论文了吧好像写了不少东西呃可以说跟你们聊聊聊这个就行了\n",
      " 我目前没有这个东西,我连这个论文都买不起啊。这篇论文是人家清华的教授发的呀还是清华大学计算机系的研究生呢\n",
      " 我不太懂这个啊,我也不太明白什么是大话梅。嗯,这个东西不了解\n",
      " 我不太清楚啊,我不太明白这个事儿。你这个东西是直接通过什么方式来获取呢还是从外部引入的呀\n",
      " 你说这个东西。本身这个机器翻译就是一个技术,它本质上就是人工智能的一种应用啊你完全没有必要去质疑它的性能的\n",
      " 那肯定就是这个东西啊,这个在大话术方面确实挺有用的。但是说什么呢?嗯你得有一个好的算法呀,没有好的算法的话你就是替代不了人类大脑,你不如人工翻译呢\n",
      " 我不太清楚啊,我不懂这个东西。你看一下这个数据吧\n",
      " 你从最基础开始学,那肯定就是需要先学习自然语言的语法。然后还要掌握一些基本的计算机编程的知识和算法知识啊,这个数据结构呀、机器学习的基础知识都不懂的话你根本理解不了这个东西了对吧\n",
      " 我呀,我本身不是啥子专家啊。我是这个人工智能领域的一名爱好者和学习者呢,你问我评价的话我不知道咋说呃这个东西就是目前我还不了解它\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    print(tokenizer.decode(output)[len(text):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
